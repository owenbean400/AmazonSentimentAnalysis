{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Amazon product review data is from https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/. The json is parsed with to a csv file. The column heads is the reviewerID, overall, reviewText_LUKE, and reviewText. overall holds the Amazon 1-5 rating. reviewText is the review text that goes with the rating. The reviewText_LUKE is the reviewText with name entities masked with <ENT>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at studio-ousia/luke-large-finetuned-conll-2003 were not used when initializing LukeForEntitySpanClassification: ['luke.embeddings.position_ids']\n",
      "- This IS expected if you are initializing LukeForEntitySpanClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LukeForEntitySpanClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 83\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(save_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     82\u001b[0m             writer \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mwriter(file)\n\u001b[0;32m---> 83\u001b[0m             writer\u001b[38;5;241m.\u001b[39mwriterow([\u001b[38;5;28mstr\u001b[39m(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreviewerID\u001b[39m\u001b[38;5;124m\"\u001b[39m]), \u001b[38;5;28mstr\u001b[39m(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall\u001b[39m\u001b[38;5;124m\"\u001b[39m]), \u001b[43msentence_LUKE_replace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreviewText\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<ENT>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m, data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreviewText\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n\u001b[1;32m     85\u001b[0m f\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[0;32mIn[1], line 35\u001b[0m, in \u001b[0;36msentence_LUKE_replace\u001b[0;34m(text, token_str)\u001b[0m\n\u001b[1;32m     33\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(text, entity_spans\u001b[38;5;241m=\u001b[39mentity_spans, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 35\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     38\u001b[0m max_logits, max_indices \u001b[38;5;241m=\u001b[39m logits[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1495'>1496</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1496'>1497</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1497'>1498</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1498'>1499</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1499'>1500</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1500'>1501</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1501'>1502</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1502'>1503</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py:1734\u001b[0m, in \u001b[0;36mLukeForEntitySpanClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, entity_ids, entity_attention_mask, entity_token_type_ids, entity_position_ids, entity_start_positions, entity_end_positions, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=1687'>1688</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=1688'>1689</a>\u001b[0m \u001b[39mentity_start_positions (`torch.LongTensor`):\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=1689'>1690</a>\u001b[0m \u001b[39m    The start positions of entities in the word token sequence.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=1729'>1730</a>\u001b[0m \u001b[39mLos Angeles LOC\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=1730'>1731</a>\u001b[0m \u001b[39m```\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=1731'>1732</a>\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=1733'>1734</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mluke(\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=1734'>1735</a>\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=1735'>1736</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=1736'>1737</a>\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=1737'>1738</a>\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=1738'>1739</a>\u001b[0m     entity_ids\u001b[39m=\u001b[39;49mentity_ids,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=1739'>1740</a>\u001b[0m     entity_attention_mask\u001b[39m=\u001b[39;49mentity_attention_mask,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=1740'>1741</a>\u001b[0m     entity_token_type_ids\u001b[39m=\u001b[39;49mentity_token_type_ids,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=1741'>1742</a>\u001b[0m     entity_position_ids\u001b[39m=\u001b[39;49mentity_position_ids,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=1742'>1743</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=1743'>1744</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=1744'>1745</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=1745'>1746</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=1746'>1747</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=1747'>1748</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=1748'>1749</a>\u001b[0m hidden_size \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlast_hidden_state\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=1750'>1751</a>\u001b[0m entity_start_positions \u001b[39m=\u001b[39m entity_start_positions\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mexpand(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, hidden_size)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1495'>1496</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1496'>1497</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1497'>1498</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1498'>1499</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1499'>1500</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1500'>1501</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1501'>1502</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1502'>1503</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py:1166\u001b[0m, in \u001b[0;36mLukeModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, entity_ids, entity_attention_mask, entity_token_type_ids, entity_position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=1162'>1163</a>\u001b[0m     entity_embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mentity_embeddings(entity_ids, entity_position_ids, entity_token_type_ids)\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=1164'>1165</a>\u001b[0m \u001b[39m# Fourth, send embeddings through the model\u001b[39;00m\n\u001b[0;32m-> <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=1165'>1166</a>\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=1166'>1167</a>\u001b[0m     word_embedding_output,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=1167'>1168</a>\u001b[0m     entity_embedding_output,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=1168'>1169</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=1169'>1170</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=1170'>1171</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=1171'>1172</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=1172'>1173</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=1173'>1174</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=1175'>1176</a>\u001b[0m \u001b[39m# Fifth, get the output. LukeModel outputs the same as BertModel, namely sequence_output of shape (batch_size, seq_len, hidden_size)\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=1176'>1177</a>\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1495'>1496</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1496'>1497</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1497'>1498</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1498'>1499</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1499'>1500</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1500'>1501</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1501'>1502</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1502'>1503</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py:806\u001b[0m, in \u001b[0;36mLukeEncoder.forward\u001b[0;34m(self, word_hidden_states, entity_hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=797'>798</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=798'>799</a>\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=799'>800</a>\u001b[0m         word_hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=802'>803</a>\u001b[0m         layer_head_mask,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=803'>804</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=804'>805</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=805'>806</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=806'>807</a>\u001b[0m         word_hidden_states,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=807'>808</a>\u001b[0m         entity_hidden_states,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=808'>809</a>\u001b[0m         attention_mask,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=809'>810</a>\u001b[0m         layer_head_mask,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=810'>811</a>\u001b[0m         output_attentions,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=811'>812</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=813'>814</a>\u001b[0m word_hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=815'>816</a>\u001b[0m \u001b[39mif\u001b[39;00m entity_hidden_states \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1495'>1496</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1496'>1497</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1497'>1498</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1498'>1499</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1499'>1500</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1500'>1501</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1501'>1502</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1502'>1503</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py:740\u001b[0m, in \u001b[0;36mLukeLayer.forward\u001b[0;34m(self, word_hidden_states, entity_hidden_states, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=737'>738</a>\u001b[0m     concat_attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=738'>739</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=739'>740</a>\u001b[0m     concat_attention_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat(self_attention_outputs[:\u001b[39m2\u001b[39;49m], dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=741'>742</a>\u001b[0m outputs \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m2\u001b[39m:]  \u001b[39m# add self attentions if we output attention weights\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=743'>744</a>\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=744'>745</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeed_forward_chunk, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_size_feed_forward, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseq_len_dim, concat_attention_output\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.10/site-packages/transformers/models/luke/modeling_luke.py?line=745'>746</a>\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import spacy\n",
    "import torch\n",
    "from tqdm import trange\n",
    "from transformers import LukeTokenizer, LukeForEntitySpanClassification\n",
    "\n",
    "# File name of Amazon product review\n",
    "file_name = \"Office_Products.json\"\n",
    "\n",
    "# File name to save csv file\n",
    "save_file = \"Office_Products Test.csv\"\n",
    "\n",
    "# Load the model checkpoint\n",
    "model = LukeForEntitySpanClassification.from_pretrained(\"studio-ousia/luke-large-finetuned-conll-2003\")\n",
    "model.eval()\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = LukeTokenizer.from_pretrained(\"studio-ousia/luke-large-finetuned-conll-2003\")\n",
    "\n",
    "# Masks name entity of the text based on LUKE identified name entitiy\n",
    "def sentence_LUKE_replace(text, token_str):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "\n",
    "    entity_spans = []\n",
    "    original_word_spans = []\n",
    "    for token_start in doc:\n",
    "        for token_end in doc[token_start.i:]:\n",
    "            entity_spans.append((token_start.idx, token_end.idx + len(token_end)))\n",
    "            original_word_spans.append((token_start.i, token_end.i + 1))\n",
    "\n",
    "    inputs = tokenizer(text, entity_spans=entity_spans, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    max_logits, max_indices = logits[0].max(dim=1)\n",
    "\n",
    "    predictions = []\n",
    "    for logit, index, span in zip(max_logits, max_indices, original_word_spans):\n",
    "        if index != 0:  # the span is not NIL\n",
    "            predictions.append((logit, span, model.config.id2label[int(index)]))\n",
    "\n",
    "    # construct an IOB2 label sequence\n",
    "    predicted_sequence = [\"O\"] * len(doc)\n",
    "    for _, span, label in sorted(predictions, key=lambda o: o[0], reverse=True):\n",
    "        if all([o == \"O\" for o in predicted_sequence[span[0] : span[1]]]):\n",
    "            predicted_sequence[span[0]] = \"B-\" + label\n",
    "            if span[1] - span[0] > 1:\n",
    "                predicted_sequence[span[0] + 1 : span[1]] = [\"I-\" + label] * (span[1] - span[0] - 1)\n",
    "\n",
    "    lst_nnp_words = []\n",
    "\n",
    "    row_continue = False\n",
    "    for token, label in zip(doc, predicted_sequence):\n",
    "        if label != \"O\":\n",
    "            if row_continue:\n",
    "                lst_nnp_words[len(lst_nnp_words) - 1] = str(lst_nnp_words[len(lst_nnp_words) - 1]) + \" \" + str(token)\n",
    "            else:\n",
    "                lst_nnp_words.append(str(token))\n",
    "            row_continue = True\n",
    "        else:\n",
    "            row_continue = False\n",
    "\n",
    "    for word in lst_nnp_words:\n",
    "        text = text.replace(word, token_str)\n",
    "    return text\n",
    "\n",
    "f = open(file_name)\n",
    "lines = f.readlines()\n",
    "\n",
    "with open(save_file, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"reviewerID\", \"overall\", \"reviewText_LUKE\", \"reviewText\"])\n",
    "\n",
    "# Go through each review text, mask the review text, and create a row in csv file\n",
    "for line in lines:\n",
    "    data = json.loads(line)\n",
    "    if \"reviewText\" in data and \"overall\" in data and \"reviewerID\" in data and len(data[\"reviewText\"]) < 150:\n",
    "        with open(save_file, \"a\", newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([str(data[\"reviewerID\"]), str(data[\"overall\"]), sentence_LUKE_replace(data[\"reviewText\"], \"<ENT>\"), data[\"reviewText\"]])\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the torch seed to get the same results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1324224321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.decoder.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.decoder.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "file_name = \"Toys_and_Games.csv\"\n",
    "model_name = \"albert-base-v2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer_LUKE = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5).to(device)\n",
    "model_luke = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5).to(device)\n",
    "\n",
    "# Read amazon product review from csv generated.\n",
    "def read_amazon_product_review(file_name):\n",
    "    df = pd.read_csv(file_name, sep=\",\", header=0)\n",
    "    dic_review = {}\n",
    "    dic_review_LUKE = {}\n",
    "    df1 = df[[\"reviewerID\", \"overall\", \"reviewText_LUKE\", \"reviewText\"]]\n",
    "    for index in range(len(df1)):\n",
    "        rating = df.loc[index, \"overall\"]\n",
    "        label = int(rating) - 1\n",
    "        \n",
    "        review_id = df.loc[index, \"reviewerID\"]\n",
    "        if label in dic_review:\n",
    "            dic_review[label][review_id] = df.loc[index, \"reviewText\"]\n",
    "        else:\n",
    "            dic_review[label] = {review_id: df.loc[index, \"reviewText\"]}\n",
    "        if label in dic_review_LUKE:\n",
    "            dic_review_LUKE[label][review_id] = df.loc[index, \"reviewText_LUKE\"]\n",
    "        else:\n",
    "            dic_review_LUKE[label] = {review_id: df.loc[index, \"reviewText_LUKE\"]}\n",
    "\n",
    "    return dic_review, dic_review_LUKE\n",
    "\n",
    "# Splits data to 80% training, 10% validation, 10% testing\n",
    "def split_data(amazon_data):\n",
    "    training = {}\n",
    "    validation = {}\n",
    "    test = {}\n",
    "\n",
    "    for label in amazon_data:\n",
    "        temp_dic = amazon_data[label]\n",
    "        lst_amazon_ids = list(temp_dic.keys())\n",
    "        train_length = int(len(lst_amazon_ids) * 0.8)\n",
    "        train_ids = lst_amazon_ids[:train_length]\n",
    "        remaining = lst_amazon_ids[train_length:]\n",
    "        test_lenght = int(len(remaining) * 0.5)\n",
    "        test_ids = remaining[:test_lenght]\n",
    "        validation_id = remaining[test_lenght:]\n",
    "\n",
    "        for amazon_id in train_ids:\n",
    "            training[temp_dic[amazon_id]] = label\n",
    "        for amazon_id in validation_id:\n",
    "            validation[temp_dic[amazon_id]] = label\n",
    "        for amazon_id in test_ids:\n",
    "            test[temp_dic[amazon_id]] = label\n",
    "\n",
    "    return training, validation, test\n",
    "\n",
    "dic_review, dic_review_LUKE = read_amazon_product_review(file_name)\n",
    "training, validation, test = split_data(dic_review)\n",
    "training_luke, validation_luke, test_luke = split_data(dic_review)\n",
    "\n",
    "class AmazonProductReviewDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_encodings = tokenizer(list(training.keys()), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(list(validation.keys()), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(list(test.keys()), truncation=True, padding=True)\n",
    "train_encodings_luke = tokenizer_LUKE(list(training_luke.keys()), truncation=True, padding=True)\n",
    "val_encodings_luke = tokenizer_LUKE(list(validation_luke.keys()), truncation=True, padding=True)\n",
    "test_encodings_luke = tokenizer_LUKE(list(test_luke.keys()), truncation=True, padding=True)\n",
    "train_dataset = AmazonProductReviewDataset(train_encodings, list(training.values()))\n",
    "val_dataset = AmazonProductReviewDataset(val_encodings, list(validation.values()))\n",
    "test_dataset = AmazonProductReviewDataset(test_encodings, list(test.values()))\n",
    "train_dataset_luke = AmazonProductReviewDataset(train_encodings_luke, list(training.values()))\n",
    "val_dataset_luke = AmazonProductReviewDataset(val_encodings_luke, list(validation.values()))\n",
    "test_dataset_luke = AmazonProductReviewDataset(test_encodings_luke, list(test.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Original Amazon Product Review Data\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da77a3e1865e498eaabc4cee1d2ac88b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/676 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a AlbertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5327, 'learning_rate': 5.207100591715976e-06, 'epoch': 0.74}\n",
      "{'train_runtime': 407.4634, 'train_samples_per_second': 26.535, 'train_steps_per_second': 1.659, 'train_loss': 0.5154461155276326, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f0796c04fd4001abd6dd1cf524f634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ng/15hc6jqj3r56lj_4ytdc6_5r0000gn/T/ipykernel_1532/2832629329.py:18: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  load_accuracy = load_metric(\"accuracy\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4499070942401886, 'eval_accuracy': 0.851109520400859, 'eval_runtime': 18.4843, 'eval_samples_per_second': 75.578, 'eval_steps_per_second': 4.761, 'epoch': 1.0}\n",
      "Testing Original Amazon Product Review Data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b9fbc6e45b4a169c8334bfbe8164c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4362846314907074, 'eval_accuracy': 0.8445706174591909, 'eval_runtime': 19.6091, 'eval_samples_per_second': 71.854, 'eval_steps_per_second': 4.539, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d0274e0151244d9be46c27283ded8c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LUKE Amazon Product Review Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad6740daea844176ac27f970c429ce45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/676 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5308, 'learning_rate': 5.207100591715976e-06, 'epoch': 0.74}\n",
      "{'train_runtime': 407.6152, 'train_samples_per_second': 26.525, 'train_steps_per_second': 1.658, 'train_loss': 0.5226578345665565, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e293120d49ca40a19a3ddb3354eed816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.45164045691490173, 'eval_accuracy': 0.8539727988546886, 'eval_runtime': 21.9879, 'eval_samples_per_second': 63.535, 'eval_steps_per_second': 4.002, 'epoch': 1.0}\n",
      "Testing LUKE Amazon Product Review Data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "724a30087280443fa66601f75e364a0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "model_name = \"fine_tune_amazon_Office_Original\"\n",
    "model_name_luke = \"fine_tune_amazon_Office_LUKE\"\n",
    "\n",
    "# Keeps the training values the same\n",
    "learning_rate = 2e-5\n",
    "per_device_train_batch_size = 16\n",
    "per_device_eval_batch_size = 16\n",
    "num_train_epochs = 1\n",
    "save_strategy = \"no\"\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "   load_accuracy = load_metric(\"accuracy\")\n",
    "  \n",
    "   logits, labels = eval_pred\n",
    "   predictions = np.argmax(logits, axis=-1)\n",
    "   accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "   return {\"accuracy\": accuracy}\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "data_collator_luke = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "   output_dir=model_name,\n",
    "   learning_rate=learning_rate,\n",
    "   per_device_train_batch_size=per_device_train_batch_size,\n",
    "   per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "   num_train_epochs=num_train_epochs,\n",
    "   save_strategy = save_strategy\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=train_dataset,\n",
    "   eval_dataset=val_dataset,\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "training_args_luke = TrainingArguments(\n",
    "   output_dir=model_name_luke,\n",
    "   learning_rate=learning_rate,\n",
    "   per_device_train_batch_size=per_device_train_batch_size,\n",
    "   per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "   num_train_epochs=num_train_epochs,\n",
    "   save_strategy = save_strategy\n",
    ")\n",
    "\n",
    "trainer_luke = Trainer(\n",
    "   model=model_luke,\n",
    "   args=training_args_luke,\n",
    "   train_dataset=train_dataset_luke,\n",
    "   eval_dataset=val_dataset_luke,\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator_luke,\n",
    "   compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Training Original Amazon Product Review Data\")\n",
    "trainer.train()\n",
    "trainer.save_model()\n",
    "print(trainer.evaluate())\n",
    "# Test dataset\n",
    "trainer.eval_dataset = test_dataset\n",
    "\n",
    "print(\"Testing Original Amazon Product Review Data\")\n",
    "print(trainer.evaluate())\n",
    "trainer.predict(test_dataset)\n",
    "\n",
    "\n",
    "print(\"Training LUKE Amazon Product Review Data\")\n",
    "trainer_luke.train()\n",
    "trainer_luke.save_model()\n",
    "print(trainer_luke.evaluate())\n",
    "trainer_luke.eval_dataset = test_dataset\n",
    "\n",
    "print(\"Testing LUKE Amazon Product Review Data\")\n",
    "print(trainer_luke.evaluate())\n",
    "trainer_luke.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Amazon product review model onto other category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a AlbertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test fine_tune_amazon_Video_Games_Original -> all_beauty.csv Original Data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b677f05bf5c248b991294015c814c618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/546 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730d1d68294f4295beb576dffd5b3e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/546 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6952040195465088, 'eval_accuracy': 0.791704857928506, 'eval_runtime': 145.3211, 'eval_samples_per_second': 60.06, 'eval_steps_per_second': 3.757}\n",
      "Test fine_tune_amazon_Video_Games_LUKE -> all_beauty.csv Original Data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc8105f6914b437eb3e3ce3d884861af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/546 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea8fd323a454bb78280d7e5f1611045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/546 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a AlbertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1054517030715942, 'eval_accuracy': 0.768102658111824, 'eval_runtime': 148.2484, 'eval_samples_per_second': 58.874, 'eval_steps_per_second': 3.683}\n",
      "Test fine_tune_amazon_Video_Games_Original -> all_beauty.csv LUKE Data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa74d96840784ba5a89b6a9a5244f705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/546 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "file_name = \"all_beauty.csv\"\n",
    "\n",
    "model_name = \"fine_tune_amazon_Video_Games_Original\"\n",
    "model_name_luke = \"fine_tune_amazon_Video_Games_LUKE\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer_luke = AutoTokenizer.from_pretrained(model_name_luke)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5).to(device)\n",
    "model_luke = AutoModelForSequenceClassification.from_pretrained(model_name_luke, num_labels=5).to(device)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "   load_accuracy = load_metric(\"accuracy\")\n",
    "  \n",
    "   logits, labels = eval_pred\n",
    "   predictions = np.argmax(logits, axis=-1)\n",
    "   accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "   return {\"accuracy\": accuracy}\n",
    "\n",
    "def read_amazon_product_review(file_name):\n",
    "    df = pd.read_csv(file_name, sep=\",\", header=0)\n",
    "    dic_review = {}\n",
    "    dic_review_LUKE = {}\n",
    "    df1 = df[[\"reviewerID\", \"overall\", \"reviewText_LUKE\", \"reviewText\"]]\n",
    "    for index in range(len(df1)):\n",
    "        rating = df.loc[index, \"overall\"]\n",
    "        label = int(rating) - 1\n",
    "        \n",
    "        review_id = df.loc[index, \"reviewerID\"]\n",
    "        if label in dic_review:\n",
    "            dic_review[label][review_id] = df.loc[index, \"reviewText\"]\n",
    "        else:\n",
    "            dic_review[label] = {review_id: df.loc[index, \"reviewText\"]}\n",
    "        if label in dic_review_LUKE:\n",
    "            dic_review_LUKE[label][review_id] = df.loc[index, \"reviewText_LUKE\"]\n",
    "        else:\n",
    "            dic_review_LUKE[label] = {review_id: df.loc[index, \"reviewText_LUKE\"]}\n",
    "\n",
    "    return dic_review, dic_review_LUKE\n",
    "\n",
    "def data_for_testing(amazon_data):\n",
    "    test = {}\n",
    "\n",
    "    for label in amazon_data:\n",
    "        temp_dic = amazon_data[label]\n",
    "        lst_amazon_ids = list(temp_dic.keys())\n",
    "\n",
    "        for amazon_id in lst_amazon_ids:\n",
    "            test[temp_dic[amazon_id]] = label\n",
    "\n",
    "    return test\n",
    "\n",
    "dic_review, dic_review_LUKE = read_amazon_product_review(file_name)\n",
    "test = data_for_testing(dic_review)\n",
    "test_luke = data_for_testing(dic_review_LUKE)\n",
    "\n",
    "class AmazonProductReviewDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "test_encodings = tokenizer(list(test.keys()), truncation=True, padding=True)\n",
    "test_encodings_luke = tokenizer(list(test_luke.keys()), truncation=True, padding=True)\n",
    "luke_test_encodings = tokenizer_luke(list(test.keys()), truncation=True, padding=True)\n",
    "luke_test_encodings_luke = tokenizer_luke(list(test_luke.keys()), truncation=True, padding=True)\n",
    "test_dataset = AmazonProductReviewDataset(test_encodings, list(test.values()))\n",
    "test_dataset_luke = AmazonProductReviewDataset(test_encodings_luke, list(test.values()))\n",
    "luke_test_dataset = AmazonProductReviewDataset(luke_test_encodings, list(test.values()))\n",
    "luke_test_dataset_luke = AmazonProductReviewDataset(luke_test_encodings_luke, list(test.values()))\n",
    "\n",
    "def testing_model(model, tokenizer, model_name, data_type_str):\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=model_name,\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=1,\n",
    "        save_strategy = \"no\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.eval_dataset = test_dataset\n",
    "\n",
    "    print(\"Test \" + model_name + \" -> \" + file_name + \" \" + data_type_str + \" Data\")\n",
    "    trainer.predict(test_dataset)\n",
    "    print(trainer.evaluate())\n",
    "\n",
    "testing_model(model, tokenizer, model_name, \"Original\")\n",
    "testing_model(model_luke, tokenizer, model_name_luke, \"Original\")\n",
    "testing_model(model, tokenizer_luke, model_name, \"LUKE\")\n",
    "testing_model(model_luke, tokenizer_luke, model_name_luke, \"LUKE\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
