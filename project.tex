%
% File ranlp2023.tex
%
%% Based on the style files for ACL-IJCNLP 2021, which were
%% Based on the style files for EMNLP 2020, which were
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{project}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\title{Affects of Named Entities on Amazon Sentiment Analysis}

\author{Anne Turmel \\
  USM / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{anne.turmel@maine.edi} \\\And
  Owen Bean \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{owen.bean@maine.edu} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  % TO DO
% Abstraction needs to be written after writing everything else
\end{abstract}

\section{Introduction}

% adjectives, adverbs, nouns, and verbs can obtain sentiment detail/ - Page 11 of 
% nouns are important for content details - Page 19
% Object is a noun, proper noun, and sometimes a verb - Page 25

Sentiment Analysis looks into extracting subjective information from a source. Some words has more weight in sentiment analysis compared to other words. With reviews from Amazon, name entities are mentioned in the review text. 

There has been plenty of models on predicting sentiment analysis of Amazon reviews. The top 3 models; accurancies unsupervised data augmentation for consistency training \cite{unsupervised}, deep pyramid convolutional neural networks for text categorization \cite{pyramid}, and disconnected recurrent neural networks for text categorization \cite{disconnect}; all has been tested with Amazon product reviews from users. Although those models accurancies are between 60\%-70\% percent, does simplifing the name enitities into one word affect Amazon sentiment analysis in positive or negative accurancies. The possiblility of removing the context of named entity affects the training and tests of sentiment analysis.

\section{Related Work}

% https://aclanthology.org/H05-1044.pdf
Specific words has neutral, positive, or negative polarity when it comes to sentiment analysis. Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis research determined a new stragety of figuring out polarity of words in senitment analysis. The research did not look into the part of speech of these words, but showed how some words can have no impact to sentiment analysis. 

Research into subjectivity compared with objectivity of a word looked into parts of speech affects of sentiment analysis from Bing Liu \cite{subject}. It has been noted from Bing Liu sentiment analysis on subjectivity that adjectives, adverbs, nouns, and verbs can obtain sentiment detail. Furthermore, an object in object extraction from sentiment is a noun, proper noun, and sometimes a verb. The paper didn't look into named entities give the impacts of training with the context of the named entities.

\section{Task}

The task is to see if named entities contains necessary information is needed for sentiment analysis in Amazon product reviews models. The difference between each similar model is one data is trained on Amazon product review while the other model is trained on same Amazon product review that replaced named entities with a mask. The goal is to compare the two models to see if removing named entities has any impacts on sentiment analysis.

\section{Method}

Amazon product review data comes with a rating and review that can be used for training. The product must parse out named entities with a replacement mask to represent there is a named entity, however, removes the context of the named entity. An example of removing named entities context can be shown.

\begin{small}
\begingroup\makeatletter\def\@currenvir{verbatim}
\verbatim
"It's just like having $100. Except I
couldn't get Walmart to accept it.
Apparently you can only use it on the
interweb at Amazon." 

Detect Amazon and Walmart as named entity,
so replaced with <NNP>

"It's just like having $100. Except I
couldn't get <NNP> to accept it. 
Apparently you can only use it on 
the interweb at <NNP>."
\end{verbatim}
\end{small}

% Picture representation
% "It's just like having $100. Except I couldn't get Walmart to accept it. Apparently you can only use it on the interweb at Amazon." -> Detect Amazon and Walmart as proper noun -> "It's just like having $100. Except I couldn't get <NNP> to accept it. Apparently you can only use it on the interweb at <NNP>."

\subsection{LUKE}

LUKE is an entity extraction model that can be used for extracting entities in an Amazon product review. LUKE-500K pre-trained data had an F1 score of 94.3 on named entity recognition from CoNLL-2003 dataset \cite{yamada-etal-2020-luke}. This gives the capability for machine to detect the named entity in a passage. Knowing where the entity names are in the passage, those words can be replaced with one mask, so the context of the named entity is not known. To conclude, LUKE will parse out the named entity before sending the review text to the model.

\subsection{albert-base-v2}

Albert is pretrained BERT model on BookCorpus \cite{DBLP:journals/corr/abs-1909-11942}. The pretrained model is used to understand the english language. The purpose of Albert is to have a pretrained model ready to for fine tuning. Reason for albert is the fine-tuned task of making decision based on whole sentences. The fine-tuning process will learn from Amazon product review text to classify the rating of the review. The rating of the review will show the sentiment analysis of the fine-tuned model.

% \subsection{Unsupervised Data Augmentation}

% Unsupervised data augmentation is a way to train the model. Can make comparsion on data with proper noun context and without.

% \subsection{Deep Pyramid Convolutional Neural Networks for Text Categorization}

% Deep Pyramid Convolutional Neural Networks for Text Categorization is another method for training a model for sentiment analysis. Can make comparsion on data with proper noun context and without.

% \subsection{FastText}

% FastText can be used in a bag of tricks for efficient text classifcation. Can make comparsion on data with proper noun context and without.

\section{Experiments}

% Need to cite https://cseweb.ucsd.edu/~jmcauley/pdfs/emnlp19a.pdf
% Data is from that source.
The Amazon product review data came from University of California, San Diego Amazon product review dataset \cite{data}. The data consists of the rating and the review text that came with the rating for training and testing. The rating from 1-5 will be used for the sentiment feeling of the Amazon review.

\subsection{Parsing Named Entities}

From each category of Amazon product review, there is two copies of the same product review. One copy has the original Amazon product review text. The second copy has the review text parsed out named entities with a mask (NNP). There is at least 10,000 Amazon product reviews for each category that was used for the models. This leaves over 1,000 review text being tested for evaluation of the model.

\subsection{Training the Models}

% Edit the model once done
The albert-base-v2 will be used for fine-tuning on detecting sentiment analysis. Each model requiring training will be done with 60\% training, 20\% validation, and 20\% testing. The models will be tested on accurancy between the Amazon product reviews untouched and the review that has named entity replaced with a mask. The only change with the models is the text has named entity parsed out. This will show how the model compares with or without the context of the proper noun.

When training with albert pretrained model, the PyTorch seed is set to 1324224321 to reduce random when training the model. Moreover, the learning rate, batch size, and number of epochs stays the same. Keeping these values the same will ensure the only change between the two models is the review text being masked or not.

\subsection{Results}

Shown in figure 1.1, the left column is the category the Amazon product review data was in. The middle column is the original unalterated product review text accurancy for testing evaluation after training. The right column is the alterate prduct review with named entity detected by LUKE to be replaced with a mask testing accurancy after training on masked review text. \\

\begin{center}
\textbf{albert-base-v2 fine-tuned accurancy}
\noindent\begin{tabular}{|l|r|r|}
\hline
 Category & Ori. Accur. & LUKE Accur. \\
\hline
Video Games (1 e.)    & 0.789       & 0.779       \\
Video Games (5 e.)    & 0.784       & 0.775       \\
Appliances (1 e.)     & 0.740       & 0.712       \\
Appliances (5 e.)     & 0.712       & 0.714       \\
Toys and Games (1 e.) & 0.853       & 0.850       \\
Toys and Games (5 e.) & 0.841       & 0.846       \\
All Beauty (1 e.)     & 0.809       & 0.801       \\
All Beauty (5 e.)     & 0.781       & 0.801       \\
\hline
\end{tabular}
\end{center}



\section{Conclusion}

Conclusion.

\bibliographystyle{plain}
\bibliography{project}

\end{document}